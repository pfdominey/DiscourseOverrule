
Updated February 3, 2021 to take into account a change in presenting results in terms of semantic similarity, vs predicted N400, which is simply 1 - semantic similarity
-----
General information

1. Installation of Anaconda 
Download for Python3.7 (https://www.anaconda.com/distribution/)


2. Installation of libraries 
On anaconda prompt run 'conda install jupyter'

To check the location of anaconda installed, run 'conda info -e'
To activate pip on anaconda, run 'activate <location of anaconda installed>'
pip install wikipedia2vec
pip install easyesn

3. Download large files:

using wikipedia2vec requires the pretrained model file
enwiki_20180420_100d.pkl
which can be downloaded from
https://wikipedia2vec.github.io/wikipedia2vec/pretrained/
http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_100d.pkl.bz2

we also created training data files for the reservoir


inputDataTraining_4k_average.npy and 
outputDataTraining_4k_average.npy

these files are available for local download via google drive
https://drive.google.com/drive/folders/1aCVvjyASIFVu-f2A-EdTDOUD30ZzcGOP?usp=sharing


4. Run code 
Go to the location of the ipynb notebook by using 'cd' command
If you run python code, just run 'python <name of sourse code>'
If you want to use jupyter notebook, run 'jupyter notebook'
On the blowser automatically displayed, click the ipynb notebook you want
In the notebook, use 'Shift+Enter' to proceed lines

-----


Exp0-reservoirAveraging.ipynb - this is the training of the reservoir to generate vector averages
	- jupyter notebook file - self explanatory
	- generates figure 1 - performace of reservoir on generating vector averages

Exp1-Chwilla-N400.ipynb 
	- jupyter notebook file
	- makes wikipedia2vec embeddings for words in pairs, and calculates the cosine similarity, and then N400 = 1 - semantic similarity
	- Generates figure 2 - ChwillaPairsBOXandHistogram.png

Exp2A-Peanut_discourse_average-N400.py
	- compares target words salted and love to the incrementally growing discourse average vector
	- generates figure 3A - Peanut-Average-Trajectory.png

Exp2B-reservoir-Peanut-N400.ipynb
	- similar to Exp2A, but now using trained reservoir to generate the discourse average vector.
	- generates figure 3B - test_averaging_reservoir.png

Exp2C-Peanut-reservoir-iterations.py
	- runs 50 seperate reservoirs - takes about 2 hours
	- generates seperate figures for each reservoir
	- generates summary data used for Figure 4 - this is saved to an npy file which is then opened in , so you can run Exp2C-Peanut-reservoir-iterations.py >> savedata and then copy and paste the data from this log file into Exp2-Figures-Peanut-res-data_analysis.jpynb

Exp2-Peanut-N400.ipynb
	- opens the data generated by the reservoir and plots figure 4

Exp3A-metusalem2012_average.py
	discourse_words_1: list of discourse words which are near the target words (partial context)
	discourse_words_1and2: list of all discourse words (whole context)
	target_word1: expected word
	target_word2: word that fit with the discourse but do not fit in the sentence
	target_word3: word that dont fit at all
	- output
	Step1: cosine similarity between discourse_words_1 and target words
	Step2: cosine similarity between discourse_words_1and2 and target words
	generates data for Figure 5


Exp3B-metusalem2012_Reservoir
	- Same as 3A but using Discourse-Reservoir to accumulate the average vector
	- generates data for figure 6 from article

Exp3C-metusalem2012_Reseroir50
	- runs the 72 metusalem scenarios with 50 reservoirs (can be slow, you can change the number of reservoirs)
	- generates data for Figure 7
	- used in Exp3N400predict.ipynb

Exp3N400predict
	- opens data files generated for Exp3 and generates figures 5-7







